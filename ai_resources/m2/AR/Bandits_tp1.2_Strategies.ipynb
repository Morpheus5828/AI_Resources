{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:51:54.781385800Z",
     "start_time": "2024-10-22T13:51:54.174920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pour que les changements dans les modules importés (bandits.py) soient pris en compte\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Pour afficher les figures dans le notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:52:00.959652300Z",
     "start_time": "2024-10-22T13:52:00.895571400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:52:01.189861500Z",
     "start_time": "2024-10-22T13:52:01.122800500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Changement des paramètres d'affichage par défaut\n",
    "mpl.rcParams['figure.figsize'] = 20, 10\n",
    "mpl.rcParams['lines.linewidth'] = 4\n",
    "mpl.rcParams['font.size'] = 24\n",
    "mpl.rcParams['lines.markersize'] = 15\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "# mpl.rcParams.find_all('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:52:05.603663900Z",
     "start_time": "2024-10-22T13:52:05.538604Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Pour utiliser la correction (prof)\n",
    "    import bandits_correction as bandits\n",
    "    print('bandits_correction importé')\n",
    "    import exp_correction\n",
    "    print('exp_correction importé')\n",
    "except:\n",
    "    # Pour utiliser le code fourni à compléter (étudiants)\n",
    "    import bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T13:52:06.657272900Z",
     "start_time": "2024-10-22T13:52:06.598217900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-arms bandit problem with Bernoulli distributions\n"
     ]
    }
   ],
   "source": [
    "# Création d'un problème avec 4 bras\n",
    "bandits_pb = bandits.BernoulliMultiArmedBanditsEnv(means = [0.1, 0.85, 0.7, 0.849])\n",
    "print(bandits_pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les agents/algorithmes de bandits: la classe abstraite `BanditAgent`\n",
    "Dans `bandits.py`, on fournit une classe abstraite `BanditAgent` générique pour les algorithmes de bandits:\n",
    "* la méthode abstraite `get_action()` renvoie l'action choisie\n",
    "* la méthode `fit_step(a, r)` prend en argument une action `a` et une récompense `r` obtenue suite à cette action pour apprendre à partir de cet exemple, par exemple en mettant à jour les estimations des valeurs des bras.  Plusieurs sous-classes sont proposées pour les différentes stratégies de résolution.\n",
    "\n",
    "La plupart des agents doivent mettre à jour le nombre de tirages $N(a)$ de chaque bras $a$ et sa récompense moyenne $Q(a)$: ces valeurs sont stockées sous forme de tableaux numpy dans les attributs `_value_estimates` et `_n_estimates`, respectivement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration pure: stratégie aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise ici la classe `RandomBanditAgent` qui implémente un algorithme de bandit qui choisit une action totalement aléatoirement, sans rien apprendre.\n",
    "\n",
    "**À faire.** Étudiez le code de cette classe (constructeur, méthodes à `get_action` et `fit_step`).\n",
    "\n",
    "**À faire sur papier.** Calculez le gain moyen de ce joueur en fonction des vraies valeurs des bras.\n",
    "\n",
    "Le code ci-dessous est une simulation classique de l'interaction entre un agent/algorithme de bandit et son environnement: choix du bras, récompense, entrainement online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'agent\n",
    "rand_agent = bandits.RandomBanditAgent(n_arms=bandits_pb.n_arms)\n",
    "# On choisit un bras et on observe la récompense, le tout 15 fois\n",
    "for i in range(15):\n",
    "    i_arm = rand_agent.get_action()  # Choix du bras par l'agent\n",
    "    r = bandits_pb.step(i_arm)  # Récompense obtenue\n",
    "    rand_agent.fit_step(action=i_arm, reward=r)  # mise à jour de l'agent\n",
    "    print('Bras {} -> récompense {}'.format(i_arm, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cherche à tracer le regret sur un grand nombre d'itérations\n",
    "n_draws = 10**4  # On pourra augmenter ce nombre à 10**5 une fois que le code fonctionne bien pour aller plus loin dans les itérations\n",
    "\n",
    "# Simulation\n",
    "rand_agent = bandits.RandomBanditAgent(n_arms=bandits_pb.n_arms)\n",
    "rewards_rand = np.empty(n_draws)\n",
    "selected_arms_rand = np.empty(n_draws)\n",
    "for i in range(n_draws):\n",
    "    i_arm = rand_agent.get_action()  # Choix du bras par l'agent\n",
    "    r = bandits_pb.step(i_arm)  # Récompense obtenue\n",
    "    rand_agent.fit_step(action=i_arm, reward=r)  # mise à jour de l'agent\n",
    "    rewards_rand[i] = r\n",
    "    selected_arms_rand[i] = i_arm\n",
    "# Calcul du regret sur une simu\n",
    "r_max = np.max(bandits_pb._true_values)  # Gain moyen du bras optimal\n",
    "cum_reward_rand = np.cumsum(rewards_rand)  # Récompenses cumulées\n",
    "regret_rand = np.arange(1, n_draws+1) * r_max - cum_reward_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On trace le gain moyen en fonction des itérations\n",
    "plt.semilogx([0, n_draws-1], [r_max, r_max], '--g', label='best average')\n",
    "plt.semilogx(cum_reward_rand / np.arange(1, n_draws+1), label='random')\n",
    "plt.semilogx([0, n_draws-1], [np.mean(bandits_pb._true_values)] * 2, ':', label='average true values')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Average gain')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les bras sélectionnés\n",
    "plt.semilogx(selected_arms_rand, label='random')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Selected arms')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le regret sur une simu\n",
    "plt.plot(regret_rand, label='random')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Regret')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation pure: stratégie greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question à chercher sur papier.** On cherche à mettre à jour les moyennes des gains de chaque bras de façon online. Pour tout $n>0$, on note $\\mu_n=\\frac{1}{n}\\sum_{i=1}^{n} x_i$ la moyenne de $n$ valeurs $x_i$, avec $\\mu_0=0$. Montrer mathématiquement que pour tout $n>1$, on a $\\mu_n = \\mu_{n-1} + \\frac{1}{n} \\left (x_n - \\mu_{n-1}\\right )$.\n",
    "\n",
    "**À faire.** En utilisant la formule précédentes, complétez la méthode `fit_step` de la classe `GreedyBanditAgent` puis implémentez sa méthode `get_action`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'agent\n",
    "greedy_agent = bandits.GreedyBanditAgent(n_arms=bandits_pb.n_arms)\n",
    "# On choisit un bras et on observe la récompense, le tout 15 fois\n",
    "for i in range(15):\n",
    "    i_arm = greedy_agent.get_action()  # Choix du bras par l'agent\n",
    "    r = bandits_pb.step(i_arm)  # Récompense obtenue\n",
    "    greedy_agent.fit_step(action=i_arm, reward=r)  # mise à jour de l'agent\n",
    "    print('Bras {} -> récompense {}'.format(i_arm, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une simulation sur un grand nombre d'itérations\n",
    "np.random.seed(3)\n",
    "greedy_agent = bandits.GreedyBanditAgent(n_arms=bandits_pb.n_arms)\n",
    "rewards_greedy = np.empty(n_draws)\n",
    "selected_arms_greedy = np.empty(n_draws)\n",
    "for i in range(n_draws):\n",
    "    i_arm = greedy_agent.get_action()  # Choix du bras par l'agent\n",
    "    r = bandits_pb.step(i_arm)  # Récompense obtenue\n",
    "    greedy_agent.fit_step(action=i_arm, reward=r)  # mise à jour de l'agent\n",
    "    rewards_greedy[i] = r\n",
    "    selected_arms_greedy[i] = i_arm\n",
    "cum_reward_greedy = np.cumsum(rewards_greedy)  # Récompenses cumulées\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On trace le gain moyen en fonction des itérations\n",
    "plt.semilogx([0, n_draws-1], [r_max, r_max], '--g', label='best average')\n",
    "plt.semilogx(cum_reward_rand / np.arange(1, n_draws+1), label='random')\n",
    "plt.semilogx(cum_reward_greedy / np.arange(1, n_draws+1), label='greedy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Average gain')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(selected_arms_rand, label='random')\n",
    "plt.semilogx(selected_arms_greedy, label='greedy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Selected arms')\n",
    "plt.legend()\n",
    "print(bandits_pb._means)\n",
    "print(greedy_agent._value_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire.** La stratégie greedy peut rester piégée dans un mauvais choix. Répétez l'expérience précédente un grand nombre de fois (50) en réinitialisant l'algorithme à chaque fois; calculer ensuite un regret empirique en faisant des moyennes par rapport à toutes les initialisations de l'algorithme et tracez-le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# À compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction\n",
    "try:\n",
    "    my_vars = dict()\n",
    "    my_vars['bandits_pb'] = bandits_pb\n",
    "    my_vars['n_draws'] = n_draws\n",
    "    my_vars['regret_rand'] = regret_rand\n",
    "    my_vars['cum_reward_rand'] = cum_reward_rand\n",
    "    exp_correction.exp_greedy(my_vars)\n",
    "except:\n",
    "    print(\"Ignorer cette cellule, elle ne sert qu'à l'enseignant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Stratégie $\\epsilon$-greedy\n",
    "**À faire.** Complétez les méthodes `fit_step` et `get_action` de la classe `EpsilonGreedyBanditAgent`, réalisez l'expérience précédente dans le cas de la stratégie $\\epsilon$-greedy et tracez les résultats de toutes les stratégies en les superposant. Vous pourrez fixer $\\epsilon=0.01$ dans un premier temps. Tester ensuite plusieurs valeurs de $\\epsilon$ telles que $\\epsilon=0.1$ et $\\epsilon=0$, superposez les courbes et commentez-les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# À compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction\n",
    "try:\n",
    "    from exp_correction import exp_epsgreedy_1\n",
    "    exp_epsgreedy_1(my_vars)\n",
    "except:\n",
    "    print(\"Ignorer cette cellule, elle ne sert qu'à l'enseignant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction\n",
    "try:\n",
    "    exp_correction.exp_epsgreedy_50(my_vars)\n",
    "except:\n",
    "    print(\"Ignorer cette cellule, elle ne sert qu'à l'enseignant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Stratégie UCB\n",
    "\n",
    "**À faire.** Complétez les méthodes `fit_step` et `get_action` de la classe `UcbBanditAgent`, réalisez l'expérience précédente dans le cas de la stratégie UCB et tracez les résultats de toutes les stratégies en les superposant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# À compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction\n",
    "try:\n",
    "    exp_correction.exp_ucb(my_vars)\n",
    "except:\n",
    "    print(\"Ignorer cette cellule, elle ne sert qu'à l'enseignant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire.** Pour UCB, tracez, en fonction des itérations, la moyenne et la borne supérieure de chaque bras en superposant les courbes sur un unique graphique (dans le même esprit que dans le cours). Analysez la figure obtenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# À compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction\n",
    "try:\n",
    "    exp_correction.exp_ucb_tracking(my_vars)\n",
    "except:\n",
    "    print(\"Ignorer cette cellule, elle ne sert qu'à l'enseignant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Stratégie Thomson Sampling\n",
    " **À faire (séance 2).** Complétez les méthodes `fit_step` et `get_action` de la classe `ThompsonSamplingAgent`, réalisez l'expérience précédente dans le cas de la stratégie Thomson Sampling et tracez les résultats de toutes les stratégies en les superposant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# À compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction\n",
    "try:\n",
    "    exp_correction.exp_ts(my_vars)\n",
    "except:\n",
    "    print(\"Ignorer cette cellule, elle ne sert qu'à l'enseignant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
