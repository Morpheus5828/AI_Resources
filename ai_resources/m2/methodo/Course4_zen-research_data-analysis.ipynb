{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791ce0d6-e151-43b1-aefa-b597bd490b2f",
   "metadata": {},
   "source": [
    "Lesson 4 - Model evaluation\n",
    "---------------------------\n",
    "\n",
    "In this notebook, we manipulate some basic statistical notions using python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mobile-reputation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T14:35:58.288308400Z",
     "start_time": "2024-10-03T14:35:54.817475200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas              # deal with dataframes -> powerful database-like functions + stats\n",
    "import numpy as np         # numerical functions (e.g. generate random samples according to a distribution)\n",
    "import scipy, scipy.stats  # statistics, notably correlation\n",
    "from matplotlib import pyplot as plt   # Make graphics (histograms) easily\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-minimum",
   "metadata": {},
   "source": [
    "## Compositionality\n",
    "\n",
    "The compositionality dataset below comes from the experiments in compositionaliyty prediction described in [this paper](https://aclanthology.org/J19-1001/). We will focus on the column called _compositionality_  which contains average annotations on a scale from 0 to 5 by about 15-20 human judges per compound noun, on a set of 180 compound nouns in French. The details of the construction of this dataset can be found [here](https://aclanthology.org/P16-2026/). The dataset contains also many other columns that we may explore later, including automatic compositionality predictions, but for the moment we will just ignore them.\n",
    "\n",
    "### 1. Reading the data\n",
    "\n",
    "We will read the full dataset from a tab-separated table file using Pandas, a very useful python library for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "peaceful-september",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-10-03T14:35:58.336354300Z",
     "start_time": "2024-10-03T14:35:58.289308800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      compound_lemma  compositionality\n0  activité_physique            4.9333\n1     année_scolaire            3.6000\n2   art_contemporain            4.6000\n3         baie_vitré            3.6364\n4           bas_côté            1.3077",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>compound_lemma</th>\n      <th>compositionality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>activité_physique</td>\n      <td>4.9333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>année_scolaire</td>\n      <td>3.6000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>art_contemporain</td>\n      <td>4.6000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>baie_vitré</td>\n      <td>3.6364</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bas_côté</td>\n      <td>1.3077</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df=pandas.read_csv('superjoined.norm.csv', sep='\\t')\n",
    "lemma_comp = results_df[['compound_lemma','compositionality']]\n",
    "lemma_comp[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b9092-9024-4114-b0f4-bc0b5079d011",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "1. Use pandas' `.sort_values()` function and python slices to obtain the 10 most and least compositional compounds in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d284a5-4b09-4a1d-b542-6fe30ad88c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-insider",
   "metadata": {},
   "source": [
    "### 2. Histogram\n",
    "\n",
    "We will now focus on the _compositionality_ column. A histogram can help us have an idea of the distribution of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22}) # Increase graphic font size \n",
    "comp = results_df['compositionality']\n",
    "comp.hist(bins=10) # you can play with bin size to see what happens (default=10)\n",
    "plt.xlabel(\"compositionality\") # It's a good idea to always label your graphics' axes\n",
    "plt.ylabel(\"nb. compounds\")\n",
    "plt.xticks(np.arange(6))       # Add a tick for every value between 0 and 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0964b24a-a631-4b90-b0d6-788ed205177f",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "  \n",
    "1. Does this look like a known probability distribution (uniform, normal...)?\n",
    "2. Add the parameter `density=True` and see what happens with the histogram. Is this an (estimated) probability distribution? Why?\n",
    "3. Play with the bin size and observe: what happens with the histogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec9006-1a48-4a32-b049-a7aea4de859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-sleep",
   "metadata": {},
   "source": [
    "### 3. Mean, standard deviation\n",
    "\n",
    "We will start by looking at some basic statistical descriptors of the `compositionality` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125e846-b295-4460-b1e7-8244446f2e78",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "1. Is the `std` value obtained the population (divided by $n$) or sample (divided by $n-1$) standard deviation? Calculate the standard deviation using your own implementation of the formula, and then compare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cad7d2-617f-4777-8cee-f902b5ee2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-boxing",
   "metadata": {},
   "source": [
    "### 4. Central limit theorem\n",
    "\n",
    "We would like to test empirically whether the central limit theorem is verified on this data. We randomly subsample $n$ times a subset of $k$ compounds, calculate the average, and then check its distribution with a histogram. Notice that the underlying distribution of the `compositionality` variable does **not** look like a normal distribution (histogram above)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100 # number of resamples, change and see what happens\n",
    "k = 30 # sample size, change and see what happens\n",
    "avg = []\n",
    "for i in range(n):\n",
    "    subsample = comp.sample(k)\n",
    "    avg.append(subsample.mean())\n",
    "avgDF = pandas.DataFrame(avg)\n",
    "avgDF.hist(bins=min(int(n/15),50))\n",
    "plt.title(\"n=\"+str(n)) # Add a title to the graphic\n",
    "plt.xlabel(\"average compositionality\")\n",
    "plt.ylabel(\"nb. samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f098d-b848-4a30-af20-7f744b3e283c",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "1. Change the values of $n$ (number of random samples) to see how the number of samples influences the shape of the histogram.\n",
    "1. Change the values of $k$ (size of random samples) to see how the size of samples influences the shape of the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cccbf21-7440-477e-a37d-a227d3a1348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-leone",
   "metadata": {},
   "source": [
    "### 5. Normal distribution standardisation\n",
    "\n",
    "Let us now standardise the average `compositionality` values sampled above so that they are centered around zero and have unit standard deviation. Notice that the values are now centered around 0, and that most (all) the data lays between -3 and +3, which corresponds to 3 unit standard deviations below/above the zero average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_avgDF = (avgDF - avgDF.mean()) / avgDF.std()\n",
    "std_avgDF.hist(bins=min(int(n/15),50))\n",
    "plt.title(\"n=\"+str(n)+\" (standardised)\")\n",
    "plt.xlabel(\"standardised avg. comp.\")\n",
    "plt.ylabel(\"nb. samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9c68bb-148b-4617-a39a-5107e698c09c",
   "metadata": {},
   "source": [
    "### 6. Compositionality and number of occurrences\n",
    "\n",
    "We would like to study the relationship between `compositionality` and compound frequency (number of occurrences in a large textual corpus), called `freq.w1&w2` in our dataset. Let us extract these two variables from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "compfreq = results_df[['compositionality','freq.w1&w2']]\n",
    "compfreq[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-creek",
   "metadata": {},
   "source": [
    "### 7. Scatter plot\n",
    "\n",
    "Let's start by visually inspecting the relation between the two quantities with a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-switzerland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(compfreq['compositionality'],compfreq['freq.w1&w2'])\n",
    "plt.xlabel(\"compositionality score\")\n",
    "plt.ylabel(\"corpus freq.\")\n",
    "plt.yticks(np.arange(0,120000,20000),map(lambda x:str(x)[:-3]+\"K\",np.arange(0,120000,20000))) # Fancy ticks using \"K\" instead of \"000\"\n",
    "plt.xticks(np.arange(6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-choir",
   "metadata": {},
   "source": [
    "We can see that maybe some relation exists, but it is not so straightforward to visualise it. This may be because the number of occurrences does not increase linearly (its distribution is [Zipfian](https://en.wikipedia.org/wiki/Zipf%27s_law)), and is easier to analyse in log domain.\n",
    "\n",
    "**Exercises**\n",
    "\n",
    "1. Remove the outliers, that is, compounds that are \"too\" frequent (above a certain threshold). Is it easier to visualise the relation between both variables now?\n",
    "2. Build a scatter plot to compare compositionality with the **logarithm** of the number of occurrences (instead of the raw number of occurrences). Is it easier to visualise the relation between both variables now?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b44fe-36c1-496f-bbfa-5dfa4dd446bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e0b88-c6e6-434f-83a6-0efe13ee00b6",
   "metadata": {},
   "source": [
    "### 8. Pearson (linear) correlation\n",
    "\n",
    "Now let us check the covariance and correlation between the `compositionality` vs. `frequency` of the compounds sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf403b88-ba3f-4a0b-b441-671c592000e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(compfreq['compositionality'],compfreq['freq.w1&w2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0abe4-8dd6-459f-89a3-50b12a613e10",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "1. Calculate the **Pearson correlation** between compositionality and the logarithm of frequency. What happens?\n",
    "2. Calculate the **covariance** (`np.cov()`) between compositionality and frequency. Then between compositionality and the logarithm of frequency. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b43fd1-db24-4bf4-9bbc-f4ca9dc4347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf5b47-e861-4dec-a14e-479eb5a15150",
   "metadata": {},
   "source": [
    "### 9. Spearman correlation\n",
    "\n",
    "_Adapted from https://medium.com/analytics-vidhya/spearmans-correlation-f34c094d99d8_\n",
    "\n",
    "We will study Spearman's rank correlation on a toy example consisting of grades in English and Maths courses. We will first calculate the correlation using the formula, and then verify it (a) via Pearson correlation of ranks and (b) via scipy's Spearman implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758871f1-6559-4727-a721-9b60bbee635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = np.array([67,89,88,90,95])\n",
    "maths = np.array([77,86,98,95,87])\n",
    "d = {'english':english, 'maths':maths}\n",
    "data = pandas.DataFrame(d)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c398020-f484-4d2c-bced-c47b004ed6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.sort_values(\"english\")\n",
    "data[\"rank_english\"]=np.arange(len(data))+1\n",
    "data=data.sort_values(\"maths\")\n",
    "data[\"rank_maths\"]=np.arange(len(data))+1\n",
    "data[\"d2\"]=(data[\"rank_english\"]-data[\"rank_maths\"])**2\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4720b-5015-49a2-b72c-6ede02f009a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(data)\n",
    "rho=1-((6*data[\"d2\"].sum())/(n*(n**2-1)))\n",
    "print(\"{:.2f}\".format(rho,\n",
    "      scipy.stats.pearsonr(data[\"rank_english\"],data[\"rank_maths\"])[0],\n",
    "      scipy.stats.spearmanr(data[\"english\"],data[\"maths\"])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0315623-990d-45dd-810b-525d9874e3f0",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "1. Do the same calculations for compositionality and frequency. Do you observe the same, that is, that all three ways of computing Spearman correlation yield the same value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d81e3-c92b-4549-aa6b-068077c4d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-coffee",
   "metadata": {},
   "source": [
    "### 10. Pearson vs. Spearman correlations\n",
    "\n",
    "Pearson's correlation coefficient can only measure linear correlation. Spearman correlation, on the other hand, measures monotonic correlation, as exemplified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0,10,150)\n",
    "exp = 3 # Change this value for higher odd values (5, 7, 9...) What happens?\n",
    "power = (x-5)**exp\n",
    "r = scipy.stats.pearsonr(x,power)\n",
    "rho = scipy.stats.spearmanr(x,power)\n",
    "print(\"Pearson's r = {:.2f}, Spearman's rho = {:.2f}\".format(r[0],rho[0]))\n",
    "plt.scatter(x,power)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-sector",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "Now let us check the different between the two correlation scores, Pearson and Spearman, for the **compositionality** vs. **frequency** data:\n",
    "1. Play with the `exp` variable above to change the form of the curve. What happens to Pearson and Spearman correlations?\n",
    "2. Calculate the Pearson and Spearman correlation between compositionality and frequency using `scipy`\n",
    "3. Calculate the Pearson and Spearman correlation compositionality and the _logarithm_ of frequency using `scipy`. What changes? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45adf0a2-c413-4b5b-af0a-ecb7ea1fb56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
